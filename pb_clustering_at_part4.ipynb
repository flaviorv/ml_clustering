{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYjVnEuIg+eGLLwHP1NcRv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flaviorv/ml_clustering/blob/main/pb_clustering_at_part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Downloads and imports"
      ],
      "metadata": {
        "id": "rwkwNtHHWQLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import spacy\n",
        "except:\n",
        "  !pip install -U spacy\n",
        "  import spacy\n",
        "\n",
        "try:\n",
        "  spacy.load('en_core_web_sm')\n",
        "except:\n",
        "  !python -m spacy download en_core_web_sm\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score"
      ],
      "metadata": {
        "id": "zcPkLEAHuJV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20 News Groups Dataset Preprocessing"
      ],
      "metadata": {
        "id": "1PQ0duJjXBJA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJhrfXknqQiQ",
        "outputId": "c4961e5f-46b5-4281-ead2-5b98fd5c6024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing document 100/11314...\n",
            "Processing document 200/11314...\n",
            "Processing document 300/11314...\n",
            "Processing document 400/11314...\n",
            "Processing document 500/11314...\n",
            "Processing document 600/11314...\n",
            "Processing document 700/11314...\n",
            "Processing document 800/11314...\n",
            "Processing document 900/11314...\n",
            "Processing document 1000/11314...\n",
            "Processing document 1100/11314...\n",
            "Processing document 1200/11314...\n",
            "Processing document 1300/11314...\n",
            "Processing document 1400/11314...\n",
            "Processing document 1500/11314...\n",
            "Processing document 1600/11314...\n",
            "Processing document 1700/11314...\n",
            "Processing document 1800/11314...\n",
            "Processing document 1900/11314...\n",
            "Processing document 2000/11314...\n",
            "Processing document 2100/11314...\n",
            "Processing document 2200/11314...\n",
            "Processing document 2300/11314...\n",
            "Processing document 2400/11314...\n",
            "Processing document 2500/11314...\n",
            "Processing document 2600/11314...\n",
            "Processing document 2700/11314...\n",
            "Processing document 2800/11314...\n",
            "Processing document 2900/11314...\n",
            "Processing document 3000/11314...\n",
            "Processing document 3100/11314...\n",
            "Processing document 3200/11314...\n",
            "Processing document 3300/11314...\n",
            "Processing document 3400/11314...\n",
            "Processing document 3500/11314...\n",
            "Processing document 3600/11314...\n",
            "Processing document 3700/11314...\n",
            "Processing document 3800/11314...\n",
            "Processing document 3900/11314...\n",
            "Processing document 4000/11314...\n",
            "Processing document 4100/11314...\n",
            "Processing document 4200/11314...\n",
            "Processing document 4300/11314...\n",
            "Processing document 4400/11314...\n",
            "Processing document 4500/11314...\n",
            "Processing document 4600/11314...\n",
            "Processing document 4700/11314...\n",
            "Processing document 4800/11314...\n",
            "Processing document 4900/11314...\n",
            "Processing document 5000/11314...\n",
            "Processing document 5100/11314...\n",
            "Processing document 5200/11314...\n",
            "Processing document 5300/11314...\n",
            "Processing document 5400/11314...\n",
            "Processing document 5500/11314...\n",
            "Processing document 5600/11314...\n",
            "Processing document 5700/11314...\n",
            "Processing document 5800/11314...\n",
            "Processing document 5900/11314...\n",
            "Processing document 6000/11314...\n",
            "Processing document 6100/11314...\n",
            "Processing document 6200/11314...\n",
            "Processing document 6300/11314...\n",
            "Processing document 6400/11314...\n",
            "Processing document 6500/11314...\n",
            "Processing document 6600/11314...\n",
            "Processing document 6700/11314...\n",
            "Processing document 6800/11314...\n",
            "Processing document 6900/11314...\n",
            "Processing document 7000/11314...\n",
            "Processing document 7100/11314...\n",
            "Processing document 7200/11314...\n",
            "Processing document 7300/11314...\n",
            "Processing document 7400/11314...\n",
            "Processing document 7500/11314...\n",
            "Processing document 7600/11314...\n",
            "Processing document 7700/11314...\n",
            "Processing document 7800/11314...\n",
            "Processing document 7900/11314...\n",
            "Processing document 8000/11314...\n",
            "Processing document 8100/11314...\n",
            "Processing document 8200/11314...\n",
            "Processing document 8300/11314...\n",
            "Processing document 8400/11314...\n",
            "Processing document 8500/11314...\n",
            "Processing document 8600/11314...\n",
            "Processing document 8700/11314...\n",
            "Processing document 8800/11314...\n",
            "Processing document 8900/11314...\n",
            "Processing document 9000/11314...\n",
            "Processing document 9100/11314...\n",
            "Processing document 9200/11314...\n",
            "Processing document 9300/11314...\n",
            "Processing document 9400/11314...\n",
            "Processing document 9500/11314...\n",
            "Processing document 9600/11314...\n",
            "Processing document 9700/11314...\n",
            "Processing document 9800/11314...\n",
            "Processing document 9900/11314...\n",
            "Processing document 10000/11314...\n",
            "Processing document 10100/11314...\n",
            "Processing document 10200/11314...\n",
            "Processing document 10300/11314...\n",
            "Processing document 10400/11314...\n",
            "Processing document 10500/11314...\n",
            "Processing document 10600/11314...\n",
            "Processing document 10700/11314...\n",
            "Processing document 10800/11314...\n",
            "Processing document 10900/11314...\n",
            "Processing document 11000/11314...\n",
            "Processing document 11100/11314...\n",
            "Processing document 11200/11314...\n",
            "Processing document 11300/11314...\n",
            "All 11314 docs have been processed!\n"
          ]
        }
      ],
      "source": [
        "# Loading dataset\n",
        "data = fetch_20newsgroups()\n",
        "x = data.data\n",
        "y = data.target\n",
        "\n",
        "# Lemmatization, stopwords and punctuation removal\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def clean(text):\n",
        "    doc = nlp(text)\n",
        "    clean_words = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and token.is_alpha:\n",
        "            clean_words.append(token.lemma_.lower())\n",
        "    return \" \".join(clean_words)\n",
        "\n",
        "cleaned_docs = []\n",
        "total_docs = len(x)\n",
        "\n",
        "for i, doc in enumerate(x):\n",
        "  cleaned_doc = clean(doc)\n",
        "  cleaned_docs.append(cleaned_doc)\n",
        "  if(i + 1) % 100 == 0:\n",
        "    print(f'Processing document {i+1}/{total_docs}...')\n",
        "  if i+1 == total_docs:\n",
        "    print(f'All {total_docs} docs have been processed!')\n",
        "\n",
        "# TFIDF\n",
        "tfidf = TfidfVectorizer( min_df=0.005, max_df=0.70)\n",
        "x_vec = tfidf.fit_transform(cleaned_docs)\n",
        "terms = tfidf.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###K-Means"
      ],
      "metadata": {
        "id": "fLnW8OpFcjeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with K-Means\n",
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters)\n",
        "clusters = kmeans.fit_predict(x_vec)\n",
        "\n",
        "# Show K-Means metrics\n",
        "print('K-Means')\n",
        "print(f'NMI {normalized_mutual_info_score(y, clusters):.2f}')\n",
        "print(f'ARI {adjusted_rand_score(y, clusters):.2f}', end='\\n\\n')\n",
        "\n",
        "# Show clusters words\n",
        "for c in range(n_clusters):\n",
        "  centroids = kmeans.cluster_centers_[c]\n",
        "  top_indices = centroids.argsort()[-10:][::-1]\n",
        "  top_words = [terms[i] for i in top_indices]\n",
        "  print(f'Cluster {c}: {top_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV9SvmH63nDk",
        "outputId": "c29c7267-2d07-4f9d-b79e-0a55b5874cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Means\n",
            "NMI 0.48\n",
            "ARI 0.16\n",
            "\n",
            "Cluster 0: ['morality', 'keith', 'objective', 'moral', 'schneider', 'allan', 'atheists', 'value', 'jon', 'system']\n",
            "Cluster 1: ['sale', 'offer', 'sell', 'condition', 'price', 'shipping', 'include', 'distribution', 'new', 'university']\n",
            "Cluster 2: ['scsi', 'ide', 'controller', 'drive', 'bus', 'isa', 'device', 'mb', 'quadra', 'mac']\n",
            "Cluster 3: ['israel', 'israeli', 'jews', 'arab', 'arabs', 'policy', 'jewish', 'israelis', 'kill', 'lebanon']\n",
            "Cluster 4: ['monitor', 'mac', 'modem', 'port', 'apple', 'mouse', 'color', 'pc', 'serial', 'problem']\n",
            "Cluster 5: ['gun', 'firearm', 'weapon', 'handgun', 'criminal', 'control', 'people', 'law', 'crime', 'police']\n",
            "Cluster 6: ['god', 'atheist', 'believe', 'hell', 'bible', 'people', 'jesus', 'faith', 'atheism', 'exist']\n",
            "Cluster 7: ['file', 'format', 'windows', 'program', 'directory', 'convert', 'image', 'ftp', 'help', 'use']\n",
            "Cluster 8: ['card', 'driver', 'video', 'vga', 'diamond', 'ati', 'vesa', 'bus', 'color', 'mode']\n",
            "Cluster 9: ['drive', 'disk', 'hard', 'floppy', 'mb', 'boot', 'problem', 'cd', 'tape', 'format']\n",
            "Cluster 10: ['player', 'hockey', 'team', 'nhl', 'play', 'league', 'stat', 'game', 'good', 'season']\n",
            "Cluster 11: ['fbi', 'batf', 'koresh', 'fire', 'atf', 'waco', 'compound', 'child', 'gas', 'write']\n",
            "Cluster 12: ['turkish', 'armenian', 'armenians', 'armenia', 'turkey', 'argic', 'turks', 'serdar', 'greek', 'genocide']\n",
            "Cluster 13: ['key', 'clipper', 'chip', 'encryption', 'escrow', 'government', 'algorithm', 'crypto', 'nsa', 'phone']\n",
            "Cluster 14: ['edu', 'georgia', 'freenet', 'virginia', 'reserve', 'university', 'cleveland', 'western', 'usa', 'michael']\n",
            "Cluster 15: ['banks', 'gordon', 'chastity', 'shameful', 'skepticism', 'intellect', 'surrender', 'pittsburgh', 'soon', 'univ']\n",
            "Cluster 16: ['posting', 'host', 'nntp', 'university', 'thank', 'write', 'article', 'distribution', 'know', 'reply']\n",
            "Cluster 17: ['space', 'nasa', 'moon', 'orbit', 'launch', 'shuttle', 'henry', 'earth', 'lunar', 'spencer']\n",
            "Cluster 18: ['jesus', 'christian', 'church', 'god', 'christians', 'christ', 'bible', 'christianity', 'people', 'believe']\n",
            "Cluster 19: ['drug', 'kid', 'article', 'clinton', 'people', 'war', 'write', 'ryan', 'take', 'think']\n",
            "Cluster 20: ['team', 'win', 'year', 'play', 'season', 'trade', 'good', 'write', 'article', 'toronto']\n",
            "Cluster 21: ['car', 'bike', 'ride', 'article', 'engine', 'write', 'dod', 'good', 'like', 'oil']\n",
            "Cluster 22: ['game', 'win', 'team', 'play', 'score', 'baseball', 'season', 'run', 'fan', 'leafs']\n",
            "Cluster 23: ['font', 'printer', 'print', 'laser', 'driver', 'windows', 'hp', 'use', 'university', 'thank']\n",
            "Cluster 24: ['window', 'manager', 'event', 'application', 'expose', 'problem', 'program', 'create', 'display', 'use']\n",
            "Cluster 25: ['motif', 'widget', 'server', 'program', 'run', 'client', 'application', 'xterm', 'sun', 'display']\n",
            "Cluster 26: ['msg', 'food', 'sensitivity', 'superstition', 'reaction', 'effect', 'study', 'eat', 'cause', 'taste']\n",
            "Cluster 27: ['windows', 'dos', 'ms', 'run', 'nt', 'window', 'file', 'program', 'version', 'use']\n",
            "Cluster 28: ['people', 'write', 'article', 'think', 'know', 'law', 'right', 'say', 'government', 'like']\n",
            "Cluster 29: ['simm', 'speed', 'radar', 'chip', 'detector', 'clock', 'ram', 'memory', 'mhz', 'vram']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LDA"
      ],
      "metadata": {
        "id": "X9gkU2z7cnmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LDA in the dataset\n",
        "lda = LatentDirichletAllocation(n_components=25)\n",
        "lda_matrix = lda.fit_transform(x_vec)\n",
        "\n",
        "# Getting the dominant topic of each document\n",
        "topics = lda_matrix.argmax(axis=1)\n",
        "\n",
        "# LDA metrics\n",
        "print('LDA')\n",
        "print(f'NMI {normalized_mutual_info_score(y, topics):.2f}')\n",
        "print(f'ARI {adjusted_rand_score(y, topics):.2f}', end='\\n\\n')\n",
        "\n",
        "# Words by topics\n",
        "topic_words = []\n",
        "for i, topic in enumerate(lda.components_):\n",
        "  top_indices = topic.argsort()[-10:][::-1]\n",
        "  top_words = [terms[i] for i in top_indices]\n",
        "  print(f'Topic {i+1}: {top_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTDmwoT60MNS",
        "outputId": "5ee37015-0b40-45b6-cc4a-7ff3acc5e5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA\n",
            "NMI 0.41\n",
            "ARI 0.17\n",
            "\n",
            "Topic 1: ['key', 'clipper', 'encryption', 'chip', 'government', 'escrow', 'nsa', 'crypto', 'system', 'security']\n",
            "Topic 2: ['card', 'driver', 'monitor', 'video', 'vga', 'diamond', 'vesa', 'color', 'mouse', 'ati']\n",
            "Topic 3: ['ring', 'testing', 'german', 'fri', 'andrew', 'corp', 'ed', 'packet', 'rush', 'university']\n",
            "Topic 4: ['orbit', 'lunar', 'gm', 'moon', 'temporary', 'solar', 'hopkins', 'josh', 'earth', 'conference']\n",
            "Topic 5: ['mellon', 'carnegie', 'pa', 'pittsburgh', 'gateway', 'engineering', 'electrical', 'host', 'nntp', 'posting']\n",
            "Topic 6: ['jewish', 'linux', 'baseball', 'space', 'ticket', 'shuttle', 'vs', 'instruction', 'sender', 'digest']\n",
            "Topic 7: ['sun', 'shaft', 'handling', 'max', 'com', 'eng', 'microsystems', 'george', 'matt', 'drive']\n",
            "Topic 8: ['fbi', 'homosexual', 'batf', 'gay', 'atf', 'clayton', 'fire', 'cramer', 'waco', 'compound']\n",
            "Topic 9: ['people', 'write', 'article', 'think', 'say', 'right', 'know', 'israel', 'law', 'state']\n",
            "Topic 10: ['georgia', 'athens', 'michael', 'artificial', 'intelligence', 'amateur', 'ai', 'institute', 'technology', 'radio']\n",
            "Topic 11: ['tower', 'cool', 'nuclear', 'polytechnic', 'craig', 'plant', 'water', 'winter', 'laboratories', 'rob']\n",
            "Topic 12: ['car', 'write', 'article', 'like', 'bike', 'posting', 'host', 'nntp', 'good', 'think']\n",
            "Topic 13: ['walker', 'mailing', 'list', 'regional', 'joe', 'corporation', 'systems', 'canada', 'jones', 'simon']\n",
            "Topic 14: ['freenet', 'cleveland', 'reserve', 'western', 'edu', 'ohio', 'case', 'mr', 'usa', 'tony']\n",
            "Topic 15: ['game', 'team', 'player', 'play', 'win', 'hockey', 'year', 'season', 'baseball', 'fan']\n",
            "Topic 16: ['gordon', 'banks', 'turkish', 'armenian', 'armenians', 'armenia', 'serdar', 'argic', 'turkey', 'turks']\n",
            "Topic 17: ['sale', 'drive', 'offer', 'sell', 'university', 'price', 'distribution', 'cd', 'controller', 'mb']\n",
            "Topic 18: ['radar', 'detector', 'ground', 'neutral', 'wire', 'van', 'receiver', 'la', 'nj', 'circuit']\n",
            "Topic 19: ['god', 'jesus', 'bible', 'christ', 'faith', 'christians', 'believe', 'sin', 'church', 'christian']\n",
            "Topic 20: ['denver', 'math', 'cell', 'unix', 'dept', 'comp', 'cs', 'sci', 'access', 'public']\n",
            "Topic 21: ['mask', 'plate', 'cry', 'seven', 'adult', 'education', 'music', 'faith', 'presumably', 'mary']\n",
            "Topic 22: ['gun', 'keith', 'firearm', 'weapon', 'handgun', 'schneider', 'allan', 'criminal', 'crime', 'atheists']\n",
            "Topic 23: ['vax', 'vnews', 'vms', 'propulsion', 'jet', 'moon', 'billion', 'news', 'prize', 'software']\n",
            "Topic 24: ['file', 'windows', 'thank', 'window', 'use', 'program', 'university', 'problem', 'host', 'posting']\n",
            "Topic 25: ['nuntius', 'useragent', 'xxdate', 'chuck', 'apr', 'sam', 'gmt', 'id', 'daily', 'tue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NMF"
      ],
      "metadata": {
        "id": "GNCrLlxtcqwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NMF on dataset\n",
        "nmf = NMF(n_components=25)\n",
        "nmf_matrix = nmf.fit_transform(x_vec)\n",
        "\n",
        "# Getting the dominant topic of each doc\n",
        "topics = nmf_matrix.argmax(axis=1)\n",
        "\n",
        "# NMF metrics\n",
        "print('NMF')\n",
        "print(f'NMI {normalized_mutual_info_score(y, topics):.2f}')\n",
        "print(f'ARI {adjusted_rand_score(y, topics):.2f}', end='\\n\\n')\n",
        "\n",
        "# Getting top words by topic\n",
        "topic_words = []\n",
        "for i, topic in enumerate(nmf.components_):\n",
        "  top_indices = topic.argsort()[-10:][::-1]\n",
        "  top_words = [terms[i] for i in top_indices]\n",
        "  print(f'Topic {i+1}: {top_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2tKFUITBib4",
        "outputId": "0404d1e5-a327-4e78-a5cd-91b7f9579bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMF\n",
            "NMI 0.45\n",
            "ARI 0.28\n",
            "\n",
            "Topic 1: ['people', 'think', 'go', 'write', 'like', 'know', 'thing', 'say', 'time', 'right']\n",
            "Topic 2: ['file', 'windows', 'dos', 'program', 'format', 'directory', 'ms', 'run', 'ftp', 'disk']\n",
            "Topic 3: ['team', 'player', 'play', 'year', 'nhl', 'hockey', 'league', 'season', 'good', 'win']\n",
            "Topic 4: ['key', 'chip', 'clipper', 'encryption', 'escrow', 'government', 'algorithm', 'crypto', 'phone', 'secret']\n",
            "Topic 5: ['drive', 'scsi', 'disk', 'ide', 'hard', 'controller', 'floppy', 'mb', 'boot', 'hd']\n",
            "Topic 6: ['mac', 'use', 'apple', 'modem', 'simm', 'port', 'speed', 'problem', 'memory', 'work']\n",
            "Topic 7: ['host', 'posting', 'nntp', 'distribution', 'university', 'world', 'lines', 'article', 'reply', 'usa']\n",
            "Topic 8: ['israel', 'israeli', 'jews', 'arab', 'jewish', 'arabs', 'peace', 'policy', 'israelis', 'lebanon']\n",
            "Topic 9: ['banks', 'gordon', 'chastity', 'shameful', 'skepticism', 'intellect', 'surrender', 'pittsburgh', 'science', 'univ']\n",
            "Topic 10: ['card', 'driver', 'video', 'vga', 'monitor', 'color', 'diamond', 'ati', 'mode', 'bus']\n",
            "Topic 11: ['space', 'nasa', 'launch', 'moon', 'orbit', 'shuttle', 'earth', 'station', 'cost', 'lunar']\n",
            "Topic 12: ['car', 'engine', 'dealer', 'buy', 'model', 'price', 'oil', 'speed', 'look', 'mile']\n",
            "Topic 13: ['gun', 'weapon', 'law', 'firearm', 'crime', 'criminal', 'control', 'handgun', 'police', 'carry']\n",
            "Topic 14: ['armenian', 'turkish', 'armenians', 'armenia', 'serdar', 'argic', 'turks', 'turkey', 'genocide', 'soviet']\n",
            "Topic 15: ['window', 'manager', 'application', 'problem', 'program', 'run', 'display', 'font', 'windows', 'screen']\n",
            "Topic 16: ['sale', 'offer', 'price', 'sell', 'new', 'condition', 'shipping', 'include', 'cd', 'distribution']\n",
            "Topic 17: ['bike', 'ride', 'dod', 'motorcycle', 'dog', 'rider', 'bmw', 'helmet', 'honda', 'advice']\n",
            "Topic 18: ['keith', 'morality', 'objective', 'system', 'moral', 'schneider', 'allan', 'jon', 'atheists', 'value']\n",
            "Topic 19: ['god', 'jesus', 'bible', 'believe', 'christian', 'faith', 'christ', 'christians', 'church', 'atheist']\n",
            "Topic 20: ['version', 'newsreader', 'tin', 'bill', 'write', 'hewlett', 'packard', 'hp', 'uk', 'national']\n",
            "Topic 21: ['thank', 'mail', 'help', 'information', 'advance', 'address', 'email', 'university', 'know', 'info']\n",
            "Topic 22: ['com', 'sun', 'homosexual', 'man', 'gay', 'clayton', 'cramer', 'sexual', 'article', 'male']\n",
            "Topic 23: ['edu', 'freenet', 'virginia', 'university', 'western', 'cleveland', 'reserve', 'georgia', 'usa', 'michael']\n",
            "Topic 24: ['game', 'win', 'run', 'score', 'baseball', 'season', 'detroit', 'play', 'playoff', 'pitch']\n",
            "Topic 25: ['msg', 'food', 'sensitivity', 'superstition', 'eat', 'effect', 'steve', 'cause', 'reaction', 'taste']\n"
          ]
        }
      ]
    }
  ]
}